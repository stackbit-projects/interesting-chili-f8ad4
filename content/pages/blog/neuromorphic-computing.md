---
title: Neuromorphic Computing
subtitle: lorem-ipsum
date: '2021-06-21'
categories: []
tags: []
excerpt: lorem-ipsum
thumb_image_alt: lorem-ipsum
image_alt: lorem-ipsum
image_position: top
seo:
  title: ''
  description: ''
  robots: []
  extra: []
  type: stackbit_page_meta
layout: post
---
![](/\_static/app-assets/images/Neuromorphic%20Computing.jpeg)

With Moore’s Law coming to an end and the ever-increasing importance of Artificial Intelligence, we are meeting a threshold that cannot be passed without making dramatic changes to our computing architecture - hence the idea for neuromorphic computing was born. Its definition is much less complicated than it sounds: taking inspiration from the brain, the designing and engineering of computer chips which use similar physics and computation to our nervous system is Neuromorphic computing. In essence, this will allow the creation of energy-efficient hardware capable of carrying out highly sophisticated tasks, especially machine learning. 



## What's special about the brain?

The reason why scientists have looked towards our brains (which we have taken for granted) is because of its unparalleled performance whilst being extremely efficient. For example, the 86 billion neurons and 1 quadrillion synapses equate to 1exaflop (10^18 operations per second) consuming only 20W, the same amount it takes to power a lightbulb. To put that to context, traditional computers require 1.6 million processors, 1.6 petabytes and 6MW to do the same. This is even translated in some cases in our everyday life, where we can recognise images and patterns within seconds whereas computers are required to be trained on machine learning models for hours having been fed hundreds and thousands of data sets. Furthermore, the way our brain works provides a solution to the limitations of the Von Neumann architecture (made in the 1940s) such as the bottleneck created when having the CPU and memory separate. This results in the bandwidth of data transfer to be limited causing latency and decreasing efficiency. Our brain avoids this issue since it has memory and processing embedded together; the cell body/soma can be seen as the CPU, axon as the data bus and the synapse as the memory. Moreover, you are able to pass a gradient of understanding between neurons giving you a lot more computational options rather than your basic yes and no in binary. 



## Brain on a chip in action

The hurdle that we have to jump is translating this biological computing to a solid-state device. For example, in order to mimic synapses (and communication via neurotransmitters), we need a single component which has memory however a standard resistor doesn’t. A potential solution to this problem is a memristor (as the name suggests it is a resistor which can retain memory - they behave in the same way as neurosynaptic cores) and so allows us to maintain the vital philosophy in neuromorphic computing that the CPU and memory should be very closely paired together. Therefore, as part of their solution for neuromorphic computing, engineers at the University of Michigan created a memristor-based computer. This was created to resolve the memory bottleneck especially in AI and by applying neuroscience principles it also allowed them to dramatically improve the performance per watt.

Another implementation of neuromorphic computing has been done by IBM, with their TrueNorth chips. These use neurosynaptic cores which have transistors to simulate programmable neurons. By 2013 they created a TrueNorth processor with 4096 cores, 1 million neurons and 256 million synapses - all of this requires 70mW, four orders of magnitude lower than a conventional computer.



## Advantages

These two implementations give us a clear understanding of the practical advantages of using this architecture. To begin with, the memory and computation is integrated which reduces latency. They operate without a clock - this allows asynchronous operations resulting in lower power dissipation for a given performance level and highest possible execution speeds. In addition, since every neuron is fundamentally a processor, each core is able to work both independently and in parallel which results in vast increases in performance and efficiency. Moreover, you are able to pass a gradient of understanding between neurons giving you a lot more computational options rather than your basic yes and no in binary Finally, individual cores can fail and the entire system can still function, improving scalability, mimicking the brain's neuroplasticity. 

The progression in computing this technology can be applied to almost any field, from genome sequencing to very sophisticated simulations and even real time machine learning. In the case of machine learning, neuromorphic computing would be incredibly significant as the neurosynaptic cores are essentially nodes in neural nets (ml models) represented through physical hardware instead of software abstraction which provides massive performance gains since the hardware architecture would be optimised for neural algorithms. The case for neuromorphic computing in AI is emphasised by the fact that a single neuron has thousands of connections (whereas a traditional cpu only has three); making a computer work (hardware) and think (software) like a brain will help us reproduce the way humans learn and adapt quickly whilst unlocking the door to achieve perfect artificial general intelligence. Furthermore, the improved computing performance could be used for brain simulations allowing us to treat diseases such as dementia which we have little knowledge about currently. These simulations result in a positive feedback loop since improved understanding of the brain will help us improve our neuromorphic architectures, which in turn improves our AI which then improves our brain simulations.



## Conclusion

Ultimately, the irony in humans using their brains to make brain simulations to make brain-like hardware so that computers can think more like humans will potentially lead to many breakthroughs, not only in the field of computer science, which we should all be excited for.
